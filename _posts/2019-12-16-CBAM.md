---
title: CBAM - Convolutional Block Attention Module
tags: MachineLearning DeepLearning XAI Attention 논문정리
header:
    theme: default
category: MachineLearing
key: 20191216_CBAM
---

**Paper link : [https://arxiv.org/abs/1807.06521](https://arxiv.org/abs/1807.06521)**



기존 BAM(Bottleneck Attention Module)과 같은 저자께서 작성한 논문



## Method

* 기존 BAM(Bottleneck Attention Module)과 유사하게 channel attention branch와 spatial attention branch로 구성
* channel attention과 spatial attention을 같이 진행하던 네트워크에서 순차적으로 진행하도록 모듈을 수정

<img src="/assets/js/post_images/image-20191217210158525.png" alt="image-20191217210158525" style="zoom:50%;"  />

* $$ F \in \mathbb{R}^{C\times H \times W} $$ 로 Feature map이 주어지면 Refined Feature map $$ F'' $$ 은 아래와 같음

$$
F' = M_c(F)\  \otimes \  F, \\
F'' = M_s(F') \ \otimes \ F',
$$

<br>

* ### **Channel Attention Module**

  * 각 채널 간 relationship을 이용하여서 channel attention map을 생성
  * 채널에서 정보를 squeeze 해서 데이터를 뽑는데 MaxPoolingrhk AvgPooling을 사용
    * BAM에서는 Avg만 썻지만 이번에는 둘다 사용 -> 나중에 실험적으로 둘다 사용하는 것이 성능이 좋음을 보임
  * 각 pooling에서 뽑은 데이터를 모으기 위해서 weight가 share 되는 shared network(MLP)를 사용하여 합침

  $$
  \begin{matrix}
  M_c(F) &=& \sigma(MLP(AvgPool(F)) + MLP(MaxPool(F))) \\
  			&=& \sigma(W_1(W_0(F^c_(avg)) + W_1(W_0(F^c_(max)))
  \end{matrix}
  $$

<br>

* ### **Spatial Attention Module**

  * Spatial한 정보를 얻기 위해서 channel과 유사하게 MaxPool, AvgPool을 사용함
  * input feature로 부터 pooling 값을 구한 뒤, 2d conv를 이용하여서 spatial한 attention map을 생성

<img src="/assets/js/post_images/image-20191217211000313.png" alt="image-20191217211000313" style="zoom:50%;" />

<br>

* ### Arrangement of attention modules**

  * 2개의 attention module을 순차적으로 attention을 적용하여 사용

<img src="/assets/js/post_images/image-20191217213155364.png" alt="image-20191217213155364" style="zoom:50%;" />



## Experiments

### Ablation Study

#### AvgPool과 MaxPool 적용에 대한 실험

<img src="/assets/js/post_images/image-20191217213345256.png" alt="image-20191217213345256" style="zoom:50%;" />

<br>

#### Spatial attention methods 비교

<img src="/assets/js/post_images/image-20191217213832251.png" alt="image-20191217213832251" style="zoom:50%;" />

<br>

#### Attentnion arrange 방식에 따른 비교

<img src="/assets/js/post_images/image-20191217213939102.png" alt="image-20191217213939102" style="zoom:50%;" />

<br>

### ImageNet Classification Test

<img src="/assets/js/post_images/image-20191217214314819.png" alt="image-20191217214314819" style="zoom:50%;" />

### Visualization Grad-CAM

#### visualization

<img src="/assets/js/post_images/image-20191217214449684.png" alt="image-20191217214449684" style="zoom:50%;" />

<img src="/assets/js/post_images/image-20191217214511663.png" alt="image-20191217214511663" style="zoom:50%;" />

#### mAP

<img src="/assets/js/post_images/image-20191217214852301.png" alt="image-20191217214852301" style="zoom:50%;" />
